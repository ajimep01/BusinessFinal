{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 8: Aprendizaje federado con agregación de gradiente cifrada\n",
    "\n",
    "En el anterior ejemplo, teníamos un \"agregador de confianza\" que era responsable de promediar las actualizaciones del modelo de varios trabajadores. Esto no es ideal porque asume que podemos encontrar a alguien lo suficientemente confiable para tener acceso a esta información confidencial. Este no es siempre el caso.\n",
    "\n",
    "Por lo tanto, en este ejemplo, usaremos cómo se puede usar SMPC para realizar la agregación de manera que no necesitemos un \"agregador de confianza\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 1: Aprendizaje federado normal\n",
    "\n",
    "Primero, mostraré un código que realiza el aprendizaje federado normal en el conjunto de datos de viviendas de Boston. Esta sección de código se divide en varias secciones.\n",
    "\n",
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for training (default: 8)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for testing (default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, metavar='M',\n",
    "                    help='SGD momentum (default: 0.0)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('../other/data/boston_housing.pickle','rb')\n",
    "((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so I'd rather not standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hookinfg PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft\n",
    "import syft as sy\n",
    "from syft.core import utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import random\n",
    "from syft.core.frameworks.torch import utils as torch_utils\n",
    "from torch.autograd import Variable\n",
    "hook = sy.TorchHook(verbose=False)\n",
    "me = hook.local_worker\n",
    "bob = sy.VirtualWorker(id=\"bob\",hook=hook, is_client_worker=False)\n",
    "alice = sy.VirtualWorker(id=\"alice\",hook=hook, is_client_worker=False)\n",
    "me.is_client_worker = False\n",
    "\n",
    "compute_nodes = [bob, alice]\n",
    "\n",
    "bob.add_workers([alice])\n",
    "alice.add_workers([bob])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enviar los datos a cada trabajador** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    target = Variable(target.float())\n",
    "    data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "            \n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred, target.float())\n",
    "        loss.backward()\n",
    "        model.get()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output, target.float(), size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/408 (0%)]\tLoss: 562.255920\n",
      "Train Epoch: 1 [80/408 (20%)]\tLoss: 551.988525\n",
      "Train Epoch: 1 [160/408 (39%)]\tLoss: 640.131042\n",
      "Train Epoch: 1 [240/408 (59%)]\tLoss: 113.095528\n",
      "Train Epoch: 1 [320/408 (78%)]\tLoss: 17.793673\n",
      "Train Epoch: 1 [400/408 (98%)]\tLoss: 36.492374\n",
      "Train Epoch: 2 [0/408 (0%)]\tLoss: 8.652925\n",
      "Train Epoch: 2 [80/408 (20%)]\tLoss: 17.827301\n",
      "Train Epoch: 2 [160/408 (39%)]\tLoss: 15.696451\n",
      "Train Epoch: 2 [240/408 (59%)]\tLoss: 16.655920\n",
      "Train Epoch: 2 [320/408 (78%)]\tLoss: 17.403423\n",
      "Train Epoch: 2 [400/408 (98%)]\tLoss: 17.845070\n",
      "Train Epoch: 3 [0/408 (0%)]\tLoss: 7.068490\n",
      "Train Epoch: 3 [80/408 (20%)]\tLoss: 12.011532\n",
      "Train Epoch: 3 [160/408 (39%)]\tLoss: 7.905748\n",
      "Train Epoch: 3 [240/408 (59%)]\tLoss: 13.399657\n",
      "Train Epoch: 3 [320/408 (78%)]\tLoss: 12.010728\n",
      "Train Epoch: 3 [400/408 (98%)]\tLoss: 10.196634\n",
      "Train Epoch: 4 [0/408 (0%)]\tLoss: 6.434999\n",
      "Train Epoch: 4 [80/408 (20%)]\tLoss: 7.154058\n",
      "Train Epoch: 4 [160/408 (39%)]\tLoss: 5.233712\n",
      "Train Epoch: 4 [240/408 (59%)]\tLoss: 11.577604\n",
      "Train Epoch: 4 [320/408 (78%)]\tLoss: 8.724213\n",
      "Train Epoch: 4 [400/408 (98%)]\tLoss: 8.727882\n",
      "Train Epoch: 5 [0/408 (0%)]\tLoss: 5.678280\n",
      "Train Epoch: 5 [80/408 (20%)]\tLoss: 4.741015\n",
      "Train Epoch: 5 [160/408 (39%)]\tLoss: 4.026522\n",
      "Train Epoch: 5 [240/408 (59%)]\tLoss: 10.108507\n",
      "Train Epoch: 5 [320/408 (78%)]\tLoss: 6.788218\n",
      "Train Epoch: 5 [400/408 (98%)]\tLoss: 9.154858\n",
      "Train Epoch: 6 [0/408 (0%)]\tLoss: 4.730551\n",
      "Train Epoch: 6 [80/408 (20%)]\tLoss: 3.897393\n",
      "Train Epoch: 6 [160/408 (39%)]\tLoss: 3.662471\n",
      "Train Epoch: 6 [240/408 (59%)]\tLoss: 8.842747\n",
      "Train Epoch: 6 [320/408 (78%)]\tLoss: 5.860409\n",
      "Train Epoch: 6 [400/408 (98%)]\tLoss: 9.349958\n",
      "Train Epoch: 7 [0/408 (0%)]\tLoss: 4.160314\n",
      "Train Epoch: 7 [80/408 (20%)]\tLoss: 3.643044\n",
      "Train Epoch: 7 [160/408 (39%)]\tLoss: 3.335357\n",
      "Train Epoch: 7 [240/408 (59%)]\tLoss: 7.844121\n",
      "Train Epoch: 7 [320/408 (78%)]\tLoss: 5.585374\n",
      "Train Epoch: 7 [400/408 (98%)]\tLoss: 8.804581\n",
      "Train Epoch: 8 [0/408 (0%)]\tLoss: 3.648461\n",
      "Train Epoch: 8 [80/408 (20%)]\tLoss: 3.807822\n",
      "Train Epoch: 8 [160/408 (39%)]\tLoss: 3.337471\n",
      "Train Epoch: 8 [240/408 (59%)]\tLoss: 7.382591\n",
      "Train Epoch: 8 [320/408 (78%)]\tLoss: 5.541363\n",
      "Train Epoch: 8 [400/408 (98%)]\tLoss: 8.410536\n",
      "Train Epoch: 9 [0/408 (0%)]\tLoss: 3.464297\n",
      "Train Epoch: 9 [80/408 (20%)]\tLoss: 3.837600\n",
      "Train Epoch: 9 [160/408 (39%)]\tLoss: 3.438318\n",
      "Train Epoch: 9 [240/408 (59%)]\tLoss: 6.993451\n",
      "Train Epoch: 9 [320/408 (78%)]\tLoss: 5.296585\n",
      "Train Epoch: 9 [400/408 (98%)]\tLoss: 8.270566\n",
      "Train Epoch: 10 [0/408 (0%)]\tLoss: 3.351248\n",
      "Train Epoch: 10 [80/408 (20%)]\tLoss: 3.812061\n",
      "Train Epoch: 10 [160/408 (39%)]\tLoss: 3.579133\n",
      "Train Epoch: 10 [240/408 (59%)]\tLoss: 6.517155\n",
      "Train Epoch: 10 [320/408 (78%)]\tLoss: 5.119539\n",
      "Train Epoch: 10 [400/408 (98%)]\tLoss: 8.209362\n",
      "Encoding 0 s 0.0 %\n",
      "Handling 0 s 0.0 %\n",
      "Execute call 0 s 0.0 %\n",
      "Total 19.16 s\n",
      "CPU times: user 18.5 s, sys: 387 ms, total: 18.9 s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = time.time()\n",
    "args.epochs = 10\n",
    "torch.encode_timer = 0\n",
    "torch.handle_call_timer = 0\n",
    "torch.execute_call_timer = 0\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Encoding', round(torch.encode_timer, 2), 's', round(torch.encode_timer/total_time*100, 2), '%')\n",
    "print('Handling', round(torch.handle_call_timer, 2), 's',  round(torch.handle_call_timer/total_time*100, 2), '%')\n",
    "print('Execute call', round(torch.execute_call_timer, 2), 's',  round(torch.execute_call_timer/total_time*100, 2), '%')\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 20.7802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seccion 2: Añadiendo Encrypted Aggregation\n",
    "\n",
    "Ahora vamos a modificar este ejemplo ligeramente para agregar gradientes usando cifrado. La pieza principal que es diferente es en realidad 1 o 2 líneas de código en la función train (). Por el momento, vamos a volver a procesar nuestros datos e inicializar un modelo para bob y alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list())\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    target = Variable(target.float())\n",
    "    data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred, target.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "\n",
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo la lógica de entrenamiento\n",
    "\n",
    "\n",
    "### Part A: Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is selecting which batch to train on\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "# update remote models\n",
    "# we could iterate this multiple times before proceeding, but we're only iterating once per worker here\n",
    "for remote_index in range(len(compute_nodes)):\n",
    "    data, target = remote_dataset[remote_index][data_index]\n",
    "    models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Encrypted Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list where we'll deposit our encrypted model average\n",
    "new_params = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each parameter\n",
    "for param_i in range(len(params[0])):\n",
    "\n",
    "    # for each worker\n",
    "    spdz_params = list()\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        \n",
    "        # select the identical parameter from each worker and copy it\n",
    "        copy_of_parameter = params[remote_index][param_i].data+0\n",
    "        \n",
    "        # since SMPC can only work with integers (not floats), we need\n",
    "        # to use Integers to store decimal information. In other words,\n",
    "        # we need to use \"Fixed Precision\" encoding.\n",
    "        # fix it's precision (read more about Fixed Precision encodings)\n",
    "        fixed_precision_param = copy_of_parameter.fix_precision()\n",
    "        \n",
    "        # now we encrypt it on the remote machine. Note that \n",
    "        # fixed_precision_param is ALREADY a pointer. Thus, when\n",
    "        # we call share, it actually encrpyts the data that the\n",
    "        # data is pointing TO. This returns a POINTER to  the \n",
    "        # MPC Shared object, which we need to fetch.\n",
    "        encrypted_param = fixed_precision_param.share(bob, alice)\n",
    "        \n",
    "        # now we fetch the pointer to the MPC shared value\n",
    "        param = encrypted_param.get()\n",
    "        \n",
    "        # save the parameter so we can average it with the same parameter\n",
    "        # from the other workers\n",
    "        spdz_params.append(param)\n",
    "\n",
    "    # average params from multiple workers, fetch them to the local machine\n",
    "    # decrypt and decode (from fixed precision) back into a floaing point number\n",
    "    new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "    \n",
    "    # save the new averaged parameter\n",
    "    new_params.append(new_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in params:\n",
    "    for param in model:\n",
    "        param.data *= 0\n",
    "\n",
    "for model in models:\n",
    "    model.get()\n",
    "\n",
    "for remote_index in range(len(compute_nodes)):\n",
    "    for param_index in range(len(params[remote_index])):\n",
    "        params[remote_index][param_index].data.set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo junto\n",
    "\n",
    "Y ahora que conocemos cada paso, podemos ponerlo todo junto en un ciclo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "\n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        # update remote models\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "        new_params = list()\n",
    "\n",
    "        for param_i in range(len(params[0])):\n",
    "\n",
    "            spdz_params = list()\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                spdz_params.append((params[remote_index][param_i].data+0).fix_precision().share(bob, alice).get())\n",
    "\n",
    "            new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "            new_params.append(new_param)\n",
    "\n",
    "        for model in params:\n",
    "            for param in model:\n",
    "                param.data *= 0\n",
    "\n",
    "        for model in models:\n",
    "            model.get()\n",
    "\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            for param_index in range(len(params[remote_index])):\n",
    "                params[remote_index][param_index].data.set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output, target.float(), size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Average loss: 545.0826\n",
      "\n",
      "2\n",
      "\n",
      "Test set: Average loss: 225.9891\n",
      "\n",
      "3\n",
      "\n",
      "Test set: Average loss: 27.1742\n",
      "\n",
      "4\n",
      "\n",
      "Test set: Average loss: 20.2662\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Average loss: 18.2706\n",
      "\n",
      "6\n",
      "\n",
      "Test set: Average loss: 17.2924\n",
      "\n",
      "7\n",
      "\n",
      "Test set: Average loss: 16.8838\n",
      "\n",
      "8\n",
      "\n",
      "Test set: Average loss: 16.6306\n",
      "\n",
      "9\n",
      "\n",
      "Test set: Average loss: 16.5443\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Average loss: 16.6038\n",
      "\n",
      "Encoding 0 s 0.0 %\n",
      "Handling 0 s 0.0 %\n",
      "Execute call 0 s 0.0 %\n",
      "Total 74.08 s\n",
      "CPU times: user 1min 12s, sys: 1.39 s, total: 1min 13s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = time.time()\n",
    "args.epochs = 10\n",
    "torch.encode_timer = 0\n",
    "torch.handle_call_timer = 0\n",
    "torch.execute_call_timer = 0\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(epoch)\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Encoding', round(torch.encode_timer, 2), 's', round(torch.encode_timer/total_time*100, 2), '%')\n",
    "print('Handling', round(torch.handle_call_timer, 2), 's',  round(torch.handle_call_timer/total_time*100, 2), '%')\n",
    "print('Execute call', round(torch.execute_call_timer, 2), 's',  round(torch.execute_call_timer/total_time*100, 2), '%')\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysyft",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
