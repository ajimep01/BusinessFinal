{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4: Aprendizaje federado con promedios de modelos\n",
    "\n",
    "Hasta ahora, entrenamos un modelo utilizando una versión muy simple de Aprendizaje Federado. Esto requería que cada propietario de datos confiara en el propietario del modelo para poder ver sus gradientes.\n",
    "\n",
    "Ahora, se mostrará cómo usar las herramientas avanzadas para permitir que un \"trabajador seguro\" en el que se confie agregue los pesos antes de que el modelo resultante final se devuelva al propietario del modelo (nosotros).\n",
    "\n",
    "De esta manera, solo el trabajador seguro puede ver qué pesos provienen de quién. Podríamos decir qué partes del modelo cambiaron, pero NO sabemos qué trabajador (bob o alice) hizo qué cambio, lo que crea una capa de privacidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import copy\n",
    "hook = sy.TorchHook()\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1: Crear los propietarios de los datos.\n",
    "\n",
    "Primero, vamos a crear dos propietarios de datos (Bob y Alice) cada uno con una pequeña cantidad de datos. También vamos a inicializar una máquina segura llamada \"secure_worker\". En la práctica, esto podría ser un hardware seguro (como el SGX de Intel) o simplemente un intermediario confiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a couple workers\n",
    "\n",
    "bob = sy.VirtualWorker(id=\"bob\")\n",
    "alice = sy.VirtualWorker(id=\"alice\")\n",
    "secure_worker = sy.VirtualWorker(id=\"secure_worker\")\n",
    "\n",
    "bob.add_workers([alice, secure_worker])\n",
    "alice.add_workers([bob, secure_worker])\n",
    "secure_worker.add_workers([alice, bob])\n",
    "\n",
    "# A Toy Dataset\n",
    "data = sy.Var(sy.FloatTensor([[0,0],[0,1],[1,0],[1,1]]))\n",
    "target = sy.Var(sy.FloatTensor([[0],[0],[1],[1]]))\n",
    "\n",
    "# get pointers to training data on each worker by\n",
    "# sending some training data to bob and alice\n",
    "bobs_data = data[0:2].send(bob)\n",
    "bobs_target = target[0:2].send(bob)\n",
    "\n",
    "alices_data = data[2:].send(alice)\n",
    "alices_target = target[2:].send(alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2: Crear nuestro modelo\n",
    "\n",
    "Para este ejemplo, vamos a entrenar con un modelo lineal simple. Podemos inicializarlo normalmente usando el constructor nn.Linear de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iniitalize A Toy Model\n",
    "model = nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3: enviar una copia del modelo a Alice y Bob\n",
    "\n",
    "Luego, debemos enviar una copia del modelo actual a Alice y Bob para que puedan realizar los pasos de aprendizaje en sus propios conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bobs_model = model.copy().send(bob)\n",
    "alices_model = model.copy().send(alice)\n",
    "\n",
    "bobs_opt = optim.SGD(params=alices_model.parameters(),lr=0.1)\n",
    "alices_opt = optim.SGD(params=alices_model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 4: Entrenar los modelos de Bob y Alice (en paralelo)\n",
    "\n",
    "Como es convencional con Aprendizaje Federado a través de esta técnica de un tercero confiable, cada propietario de datos primero entrena su modelo para varias iteraciones a nivel local antes de que los modelos se promedien juntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    # Train Bob's Model\n",
    "    bobs_opt.zero_grad()\n",
    "    bobs_pred = bobs_model(bobs_data)\n",
    "    bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "    bobs_loss.backward()\n",
    "\n",
    "    bobs_opt.step()\n",
    "    bobs_loss = bobs_loss.get().data[0]\n",
    "\n",
    "    # Train Alice's Model\n",
    "    alices_opt.zero_grad()\n",
    "    alices_pred = alices_model(alices_data)\n",
    "    alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "    alices_loss.backward()\n",
    "\n",
    "    alices_opt.step()\n",
    "    alices_loss = alices_loss.get().data[0]\n",
    "    alices_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 5: Enviar ambos modelos actualizados a un trabajador seguro\n",
    "\n",
    "Ahora que cada propietario de datos tiene un modelo parcialmente entrenado, es hora de promediarlos de manera segura. Esto lo logramos indicando a Alice y Bob que envíen su modelo al servidor seguro (confiable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alices_model.move(secure_worker)\n",
    "bobs_model.move(secure_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 6: Promedio de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, el último paso para esta etapa de entrenamiento es promediar los modelos entrenados de Bob y Alice y luego usarlos para establecer los valores de nuestro \"modelo\" global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight.data.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\n",
    "model.bias.data.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\n",
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetir proceso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob:0.014401242136955261 Alice:0.021205957978963852\n",
      "Bob:0.005423392169177532 Alice:0.011188073083758354\n",
      "Bob:0.0018657767213881016 Alice:0.0051170047372579575\n",
      "Bob:0.0005638035945594311 Alice:0.0022938279435038567\n",
      "Bob:0.00013662339188158512 Alice:0.001032400643453002\n",
      "Bob:2.3314680220209993e-05 Alice:0.00046972226118668914\n",
      "Bob:9.560068065184169e-06 Alice:0.00021691148867830634\n",
      "Bob:1.931003134814091e-05 Alice:0.00010205955186393112\n",
      "Bob:2.9013930543442257e-05 Alice:4.9136768211610615e-05\n",
      "Bob:3.363085852470249e-05 Alice:2.4316530470969155e-05\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "worker_iters = 5\n",
    "\n",
    "for a_iter in range(iterations):\n",
    "    \n",
    "    bobs_model = model.copy().send(bob)\n",
    "    alices_model = model.copy().send(alice)\n",
    "\n",
    "    bobs_opt = optim.SGD(params=bobs_model.parameters(),lr=0.1)\n",
    "    alices_opt = optim.SGD(params=alices_model.parameters(),lr=0.1)\n",
    "\n",
    "    for wi in range(worker_iters):\n",
    "\n",
    "        # Train Bob's Model\n",
    "        bobs_opt.zero_grad()\n",
    "        bobs_pred = bobs_model(bobs_data)\n",
    "        bobs_loss = ((bobs_pred - bobs_target)**2).sum()\n",
    "        bobs_loss.backward()\n",
    "\n",
    "        bobs_opt.step()\n",
    "        bobs_loss = bobs_loss.get().data[0]\n",
    "\n",
    "        # Train Alice's Model\n",
    "        alices_opt.zero_grad()\n",
    "        alices_pred = alices_model(alices_data)\n",
    "        alices_loss = ((alices_pred - alices_target)**2).sum()\n",
    "        alices_loss.backward()\n",
    "\n",
    "        alices_opt.step()\n",
    "        alices_loss = alices_loss.get().data[0]\n",
    "    \n",
    "    alices_model.move(secure_worker)\n",
    "    bobs_model.move(secure_worker)\n",
    "    \n",
    "    model.weight.data.set_(((alices_model.weight.data + bobs_model.weight.data) / 2).get())\n",
    "    model.bias.data.set_(((alices_model.bias.data + bobs_model.bias.data) / 2).get())\n",
    "    \n",
    "    print(\"Bob:\" + str(bobs_loss) + \" Alice:\" + str(alices_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, nos aseguramos de que nuestro modelo resultante haya aprendido correctamente, por lo que lo evaluamos en un conjunto de datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(data)\n",
    "loss = ((preds - target) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0233\n",
      " 0.0231\n",
      " 0.9699\n",
      " 0.9697\n",
      "[syft.core.frameworks.torch.tensor.FloatTensor of size 4x1]\n",
      "\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 1\n",
      "[syft.core.frameworks.torch.tensor.FloatTensor of size 4x1]\n",
      "\n",
      "0.0028978544287383556\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "print(target)\n",
    "print(loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy example, the averaged model is underfitting relative to a plaintext model trained locally would behave, however we were able to train it without exposing each worker's training data.  We were also able to aggregate the updated models from each worker on a trusted aggregator to prevent data leakage to the model owner.\n",
    "\n",
    "In a future tutorial, we'll aim to do our trusted aggregation directly with the gradients, so that we can update the model with better gradient estimates and arrive at a stronger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysyft",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
